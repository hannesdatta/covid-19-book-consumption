---
title: "data_documentation"
output:
  html_document:
    df_print: paged
date: "2022-08-23"
---
# General Q&A

- What is the data source and the motivation behind the same?

To examine the impact of COVID-19 restrictions on book consumption, book reading data was collected from Goodreads. 

Goodreads is the world’s largest book reading community, with 125 million registered users as of 2022. The website was launched in January 2007. 

- How was the data obtained?

The data was web scraped from the Goodreads web site using Python’s “requests” package and the scraped data from each page is transformed into a useful list of information using the “beautifulsoup” package. 

- On what basis were the users/observations selected ?

The users were collected from country-specific subgroups on Goodreads. This allows for coverage of a wider variety of users simultaneously and provide an indication of the country where each user lives. This enables one to match users to the country-specific COVID-19 pandemic measures they likely faced. Also, here it is assumed that a user’s country group is most likely the country where the user lives.

- How many country-specific groups were chosen?

A total of 31 largest country-specific subgroups were selected with at least 100 members.

- On what basis was a particular subgroup chosen ?

While for many countries there are multiple groups, this study includes only groups with the highest number of members. 

- What was the total duration of the data collection process?

The time taken to scrape data for 31 country-specific subgroups was limited to four months. 

- What were some limitations in the data collection process(if any)? How is it addressed?

Goodreads limits the number of members that can be viewed to 3000. This issue was addressed by automating the process with a Python while-loop that searches for letters and letter combinations until at least 99% of a group’s members are collected and no new users are discovered. 

- Were any observations excluded? 

Private users and users with zero books on their bookshelves were excluded.

- Were there any missing data?
    - 58.8% of books have no date started
    - 49.9% of books have no date finished
    - Published date is missing for 39.9% and year published is missing for 6% of books
    - Rating is missing for 13.5% of books
    
- Were there any outliers/infeasible data?
    - Age: Observations with age under 18 and over 99 were removed, as it is assumed that people of such an high age are not likely to use Goodreads and Goodreads automatically sets the profiles of underaged(below 18) users to private.
    - Start and finish dates of books: The acceptable dates are limited to 70 years before Goodreads was founded(1-1-1937) and until the last book was scraped(5-5-2022). All observations outside this range are removed.
    - Post scraping date: Since the scraping process began on Jan 1, 2022, all data uploaded after this data was removed to avoid collecting biased data for users that were scraped relatively early opposed to those scraped at the end of scraping period.
    - No. of pages: Book length exceeding 10,000 and those with 0 pages are removed.
    - First day of activity: When one registers with Goodreads, the site asks you to enter all books you have read before which might result in high number of books added on the first day of activity. Hence, these observations are removed.
- What are the final number of observations for users and book data collected?

99,641 unique users and 14,848,487 book reading records.

- What is the level of aggregation of the data?

Data is aggregated on a weekly level to remove the effect that different days of the week could have on book reading.

- How were the COVID-19 restrictions operationalized?

The OxCGRT stringency index is used. This index aggregates the stringency of eight frequently used containment and closure policies and the presence of COVID-19 information compaigns into a number of 0 and 100. For all weeks prior to Jan 1, 2020, the COVID-19 stringency index is set to 0.

# Directory structure

<img width="1056" alt="data-documentation" src="https://user-images.githubusercontent.com/67949747/186224463-47dd7e35-8d20-4769-80d6-7ef8fbf1a679.png">

# (Final) Data composition 

Dataset name: books_cleaned.csv

Short description: This is the final dataset containing book level data

Row count: `14848458`

Column count: 23

Size of dataset: 2.3 GB

Column names with description:

| variable | description | questions |
| --- | --- | --- |
| reader id | unique ID of the user |  |
| book url | URL to the book |  |
| date_started | the optional user-added date of when the user started reading the book  | max = 3645-02-02 how? |
| date_read | the optional user-added date of when the user finished reading the book | 3645-02-02 how is max so big? |
| date_added | the system-generated date of when the user added this book to their read shelf |  |
| user rating | the rating score the user assigned to this book, ranging from 1 to 5 |  |
| num_pages | the number of pages in the book |  |
| avg_rating | the average rating score that the book received by other readers, ranging from 0 to 5 |  |
| num_ratings | the number of times the read book has been rated by other readers |  |
| date_pub | the exact date when the book was published |  |
| read_count | the number of times this book was read by the user  |  |
| dummy |  | what does this indicate? |
| date_started_month |  | how is this computed? |
| date_read_month |  | same as above |
| year_pub | the year the book was published |  |
| read_time_days |  | how is this computed? |
| read_time_months |  | how is this computed? |
| age_of_book |  | how is this computed? |
| nostalgic | the number of books with a publication year 10 to 80 years before the date added, divided by the number of books added in a week. This variable is only based on the books for which the publication year is available |  |
| age_of_book_exact |  | how is this computed |
| recent |  | how is this computed? |

Dataset name: users_cleaned.csv

Short description: This is the final dataset containing user level data

Row count: `99641`

Column count: 18

Size of dataset: 11.7 MB

Column names with description:

| variable | description |  |
| --- | --- | --- |
| Nrments |  |  |
| Country | the country subgroup the user is a member of. If one is a member of multiple country groups, this variable is set to “unclear”. |  |
| Nr.Ratings | no.of ratings given by the specific user to all the books in the bookshelf in total | |
| Avg.Rating | the average rating the user has given to the books on their shelf; this value ranges from 0 to 5 |  |
| Nr.Reviews | no.of reviews given by the specific user to all the books in the bookshelf in total | |
| Nr.Books.Read | the number of books on the user’s bookshelf |  |
| Nr.Friends | the no. of friends on the user's profile |  |
| user_id | unique ID of the user |  |
| Age | self-reported age of the user |  |
| Male | gender dummy where 1=male and 0=female |  |
| Joined | the month and year in which the user joined Goodreads |  |
| Last | the month and year the user was last active on Goodreads |  |
| days_active |  | how is it computed? |
| books_per_day |  | how is it computed? |
| totalbooks_left |  | how is it computed? |
| dummy |  | what does this variable indicate? |

Dataset name: covid_stringency_prepared.csv

Short description: This dataset contains info on the OxCGRT stringency index for 31 countries

Row count: 105

Column count: 34

Size of dataset: 29.7 KB

Column names with description:
| variable | description |
| --- | --- |
| first_day_of_week | what does this indicate? |
| United Arab Emirates |  |
| Argentina |  |
| Australia |  |
| Austria |  |
| Bangladesh |  |
| Bulgaria |  |
| Egypt |  |
| Estonia |  |
| Finland |  |
| United Kingdom |  |
| Indonesia |  |
| India |  |
| Italy |  |
| Lithuania |  |
| Latvia |  |
| Mexico |  |
| Malaysia |  |
| Netherlands |  |
| Norway |  |
| Nepal |  |
| Pakistan |  |
| Panama |  |
| Philippines |  |
| Poland |  |
| Portugal |  |
| Romania |  |
| Russia |  |
| Singapore |  |
| Sweden |  |
| Turkey |  |
| Venezuela |  |
| Average |  |


